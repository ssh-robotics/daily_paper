# ğŸ¤– å…·èº«æ™ºèƒ½/æœºå™¨äººå­¦æœ¯æ—¥æŠ¥ (2026-02-26)

## ğŸ† ç²¾é€‰è®ºæ–‡ (Selective)

### 1. Biomechanical Comparisons Reveal Divergence of Human and Humanoid Gaits
- **Score:** 45
- **Why:** (ç”± AI ç”Ÿæˆæ·±åº¦åˆ†æ...)
- **What:** It remains challenging to achieve human-like locomotion in legged robots due to fundamental discrepancies between biological and mechanical structures. Although imitation learning has emerged as a promising approach for generating natural robotic movements, simply replicating joint angle trajectories fails to capture the underlying principles of human motion. This study proposes a Gait Divergence Analysis Framework (GDAF), a unified biomechanical evaluation framework that systematically quantifi...
- ğŸ“„ [arXiv](https://arxiv.org/abs/2602.21666v1) | ğŸ“¥ [PDFé¢„è§ˆ](https://arxiv.org/pdf/2602.21666v1.pdf)

---

### 2. Primary-Fine Decoupling for Action Generation in Robotic Imitation
- **Score:** 42
- **Why:** (ç”± AI ç”Ÿæˆæ·±åº¦åˆ†æ...)
- **What:** Multi-modal distribution in robotic manipulation action sequences poses critical challenges for imitation learning. To this end, existing approaches often model the action space as either a discrete set of tokens or a continuous, latent-variable distribution. However, both approaches present trade-offs: some methods discretize actions into tokens and therefore lose fine-grained action variations, while others generate continuous actions in a single stage tend to produce unstable mode transitions...
- ğŸ“„ [arXiv](https://arxiv.org/abs/2602.21684v1) | ğŸ“¥ [PDFé¢„è§ˆ](https://arxiv.org/pdf/2602.21684v1.pdf)

---

### 3. Self-Correcting VLA: Online Action Refinement via Sparse World Imagination
- **Score:** 36
- **Why:** (ç”± AI ç”Ÿæˆæ·±åº¦åˆ†æ...)
- **What:** Standard vision-language-action (VLA) models rely on fitting statistical data priors, limiting their robust understanding of underlying physical dynamics. Reinforcement learning enhances physical grounding through exploration yet typically relies on external reward signals that remain isolated from the agent's internal states. World action models have emerged as a promising paradigm that integrates imagination and control to enable predictive planning. However, they rely on implicit context mode...
- ğŸ“„ [arXiv](https://arxiv.org/abs/2602.21633v1) | ğŸ“¥ [PDFé¢„è§ˆ](https://arxiv.org/pdf/2602.21633v1.pdf)

---

### 4. Joint-Aligned Latent Action: Towards Scalable VLA Pretraining in the Wild
- **Score:** 35
- **Why:** (ç”± AI ç”Ÿæˆæ·±åº¦åˆ†æ...)
- **What:** Despite progress, Vision-Language-Action models (VLAs) are limited by a scarcity of large-scale, diverse robot data. While human manipulation videos offer a rich alternative, existing methods are forced to choose between small, precisely-labeled datasets and vast in-the-wild footage with unreliable hand tracking labels. We present JALA, a pretraining framework that learns Jointly-Aligned Latent Actions. JALA bypasses full visual dynamic reconstruction, instead learns a predictive action embeddin...
- ğŸ“„ [arXiv](https://arxiv.org/abs/2602.21736v1) | ğŸ“¥ [PDFé¢„è§ˆ](https://arxiv.org/pdf/2602.21736v1.pdf)

---

### 5. LiLo-VLA: Compositional Long-Horizon Manipulation via Linked Object-Centric Policies
- **Score:** 33
- **Why:** (ç”± AI ç”Ÿæˆæ·±åº¦åˆ†æ...)
- **What:** General-purpose robots must master long-horizon manipulation, defined as tasks involving multiple kinematic structure changes (e.g., attaching or detaching objects) in unstructured environments. While Vision-Language-Action (VLA) models offer the potential to master diverse atomic skills, they struggle with the combinatorial complexity of sequencing them and are prone to cascading failures due to environmental sensitivity. To address these challenges, we propose LiLo-VLA (Linked Local VLA), a mo...
- ğŸ“„ [arXiv](https://arxiv.org/abs/2602.21531v1) | ğŸ“¥ [PDFé¢„è§ˆ](https://arxiv.org/pdf/2602.21531v1.pdf)

---

## ğŸ‘€ ä»Šæ—¥å…³æ³¨ (Watch List)

- **Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes** (Score: 6) [Link](https://arxiv.org/abs/2602.22197v1)
- **Are Foundation Models the Route to Full-Stack Transfer in Robotics?** (Score: 6) [Link](https://arxiv.org/abs/2602.22001v1)
